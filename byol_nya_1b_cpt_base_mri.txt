2026-01-26 06:44:08,717 - INFO - üöÄ Starting 27 evaluations (1 models √ó 27 tasks)
2026-01-26 06:44:08,717 - INFO - üìÇ Output: results
2026-01-26 06:44:08,717 - INFO - üñ•Ô∏è  GPUs: 3
2026-01-26 06:44:08,717 - INFO - ============================================================
2026-01-26 06:44:08,717 - INFO - Model: byol_mri_1b_cpt (/home/whamidouche/ssdprivate/BYOL/weights/byol_mri_1b_cpt)
2026-01-26 06:44:08,717 - INFO - Task: global_mmlu_en,global_mmlu_mri
2026-01-26 06:44:08,717 - INFO - Few-shot: 5
2026-01-26 06:44:08,717 - INFO - Chat template: False
2026-01-26 06:44:08,717 - INFO - ============================================================
2026-01-26 06:44:08,717 - INFO - Command: /home/whamidouche/conda_envs/byol_eval/bin/python -m lm_eval --model hf --model_args pretrained=/home/whamidouche/ssdprivate/BYOL/weights/byol_mri_1b_cpt,dtype=bfloat16,trust_remote_code=true --tasks global_mmlu_en,global_mmlu_mri --output_path results/byol_mri_1b_cpt_global_mmlu_en_global_mmlu_mri_20260126_064408 --batch_size auto:4 --num_fewshot 5 --include_path eval/tasks
2026-01-26:06:44:14 INFO     [__main__:347] Including path: eval/tasks
2026-01-26:06:44:16 INFO     [__main__:440] Selected Tasks: ['global_mmlu_en', 'global_mmlu_mri']
2026-01-26:06:44:16 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2026-01-26:06:44:16 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/home/whamidouche/ssdprivate/BYOL/weights/byol_mri_1b_cpt', 'dtype': 'bfloat16', 'trust_remote_code': True}
2026-01-26:06:44:17 INFO     [models.huggingface:137] Using device 'cuda'
The tokenizer you are loading from '/home/whamidouche/ssdprivate/BYOL/weights/byol_mri_1b_cpt' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
2026-01-26:06:44:18 INFO     [models.huggingface:382] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}
`torch_dtype` is deprecated! Use `dtype` instead!
The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
2026-01-26:06:44:19 INFO     [models.huggingface:226] Model type is 'gemma3_text', part of the Gemma family--a BOS token will be used as Gemma underperforms without it.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/whamidouche/ssd-datadrive/ssdprivate/ghacheme/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/home/whamidouche/ssd-datadrive/ssdprivate/ghacheme/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/whamidouche/ssd-datadrive/ssdprivate/ghacheme/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/whamidouche/ssd-datadrive/ssdprivate/ghacheme/lm-evaluation-harness/lm_eval/evaluator.py", line 264, in simple_evaluate
    task_dict = get_task_dict(
                ^^^^^^^^^^^^^^
  File "/home/whamidouche/ssd-datadrive/ssdprivate/ghacheme/lm-evaluation-harness/lm_eval/tasks/__init__.py", line 635, in get_task_dict
    task_name_from_string_dict = task_manager.load_task_or_group(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/whamidouche/ssd-datadrive/ssdprivate/ghacheme/lm-evaluation-harness/lm_eval/tasks/__init__.py", line 426, in load_task_or_group
    collections.ChainMap(
  File "/home/whamidouche/ssd-datadrive/ssdprivate/ghacheme/lm-evaluation-harness/lm_eval/tasks/__init__.py", line 428, in <lambda>
    lambda task: self._load_individual_task_or_group(task),
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/whamidouche/ssd-datadrive/ssdprivate/ghacheme/lm-evaluation-harness/lm_eval/tasks/__init__.py", line 410, in _load_individual_task_or_group
    group_name: dict(collections.ChainMap(*map(fn, reversed(subtask_list))))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/whamidouche/ssd-datadrive/ssdprivate/ghacheme/lm-evaluation-harness/lm_eval/tasks/__init__.py", line 326, in _load_individual_task_or_group
    return _load_task(task_config, task=name_or_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/whamidouche/ssd-datadrive/ssdprivate/ghacheme/lm-evaluation-harness/lm_eval/tasks/__init__.py", line 286, in _load_task
    task_object = ConfigurableTask(config=config)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/whamidouche/ssd-datadrive/ssdprivate/ghacheme/lm-evaluation-harness/lm_eval/api/task.py", line 863, in __init__
    self.download(self.config.dataset_kwargs)
  File "/home/whamidouche/ssd-datadrive/ssdprivate/ghacheme/lm-evaluation-harness/lm_eval/api/task.py", line 991, in download
    self.dataset = datasets.load_dataset(
                   ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/whamidouche/conda_envs/byol_eval/lib/python3.12/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/home/whamidouche/conda_envs/byol_eval/lib/python3.12/site-packages/datasets/load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/whamidouche/conda_envs/byol_eval/lib/python3.12/site-packages/datasets/load.py", line 1497, in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
  File "/home/whamidouche/conda_envs/byol_eval/lib/python3.12/site-packages/datasets/load.py", line 913, in get_module
    data_files = DataFilesDict.from_patterns(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/whamidouche/conda_envs/byol_eval/lib/python3.12/site-packages/datasets/data_files.py", line 689, in from_patterns
    else DataFilesList.from_patterns(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/whamidouche/conda_envs/byol_eval/lib/python3.12/site-packages/datasets/data_files.py", line 582, in from_patterns
    resolve_pattern(
  File "/home/whamidouche/conda_envs/byol_eval/lib/python3.12/site-packages/datasets/data_files.py", line 383, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/whamidouche/ssdprivate/BYOL/eval/data/mmlu_lite_dev_english2maori_google_translated.jsonl'
2026-01-26 06:44:32,987 - ERROR - ‚ùå Failed: byol_mri_1b_cpt on global_mmlu_en,global_mmlu_mri
2026-01-26 06:44:32,987 - INFO - ============================================================
2026-01-26 06:44:32,987 - INFO - Model: byol_mri_1b_cpt (/home/whamidouche/ssdprivate/BYOL/weights/byol_mri_1b_cpt)
2026-01-26 06:44:32,987 - INFO - Task: xwinograd_mri
2026-01-26 06:44:32,987 - INFO - Few-shot: 0
2026-01-26 06:44:32,988 - INFO - Chat template: False
2026-01-26 06:44:32,988 - INFO - ============================================================
2026-01-26 06:44:32,988 - INFO - Command: /home/whamidouche/conda_envs/byol_eval/bin/python -m lm_eval --model hf --model_args pretrained=/home/whamidouche/ssdprivate/BYOL/weights/byol_mri_1b_cpt,dtype=bfloat16,trust_remote_code=true --tasks xwinograd_mri --output_path results/byol_mri_1b_cpt_xwinograd_mri_20260126_064432 --batch_size auto:4 --num_fewshot 0 --include_path eval/tasks
2026-01-26:06:44:38 INFO     [__main__:347] Including path: eval/tasks
2026-01-26:06:44:40 INFO     [__main__:440] Selected Tasks: ['xwinograd_mri']
2026-01-26:06:44:40 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2026-01-26:06:44:40 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/home/whamidouche/ssdprivate/BYOL/weights/byol_mri_1b_cpt', 'dtype': 'bfloat16', 'trust_remote_code': True}
2026-01-26:06:44:40 INFO     [models.huggingface:137] Using device 'cuda'
The tokenizer you are loading from '/home/whamidouche/ssdprivate/BYOL/weights/byol_mri_1b_cpt' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
2026-01-26:06:44:41 INFO     [models.huggingface:382] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}
`torch_dtype` is deprecated! Use `dtype` instead!
The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
